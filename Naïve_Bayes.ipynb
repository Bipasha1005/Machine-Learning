{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "xF82EzJTdkYV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target)\n",
        "\n",
        "print(\"Iris Dataset (first 5 rows of features):\")\n",
        "print(X.head())\n",
        "print(\"\\nTarget Variable (first 5 rows):\")\n",
        "print(y.head())\n",
        "print(f\"\\nTarget Names: {iris.target_names}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBB4AjhndkWS",
        "outputId": "9cb10266-546d-4106-83c5-ff27f3c2ef6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris Dataset (first 5 rows of features):\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0                5.1               3.5                1.4               0.2\n",
            "1                4.9               3.0                1.4               0.2\n",
            "2                4.7               3.2                1.3               0.2\n",
            "3                4.6               3.1                1.5               0.2\n",
            "4                5.0               3.6                1.4               0.2\n",
            "\n",
            "Target Variable (first 5 rows):\n",
            "0    0\n",
            "1    0\n",
            "2    0\n",
            "3    0\n",
            "4    0\n",
            "dtype: int64\n",
            "\n",
            "Target Names: ['setosa' 'versicolor' 'virginica']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Prepare the Data (Optional Scaling)\n",
        "# While Naive Bayes doesn't strictly require scaling, it's generally harmless and can sometimes help with numerical stability, especially if features have vastly different scales.\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "print(\"Scaled Iris Data (first 5 rows):\")\n",
        "print(X_scaled_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1vTXMNLdkTS",
        "outputId": "b39e1717-055e-40b7-a6fd-26aafcc9c6b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled Iris Data (first 5 rows):\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0          -0.900681          1.019004          -1.340227         -1.315444\n",
            "1          -1.143017         -0.131979          -1.340227         -1.315444\n",
            "2          -1.385353          0.328414          -1.397064         -1.315444\n",
            "3          -1.506521          0.098217          -1.283389         -1.315444\n",
            "4          -1.021849          1.249201          -1.340227         -1.315444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Split the Data\n",
        "# Using a common split size, e.g., 70% train, 30% test\n",
        "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training data shape: {X_train_scaled.shape}, {y_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test_scaled.shape}, {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP583I_LdkNQ",
        "outputId": "ced033d8-5426-4708-a02d-06f6392da8c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (105, 4), (105,)\n",
            "Testing data shape: (45, 4), (45,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Train the Na誰ve Bayes Model\n",
        "# GaussianNB is used for continuous data, assuming features follow a Gaussian distribution.\n",
        "gnb_model = GaussianNB()\n",
        "gnb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Na誰ve Bayes (GaussianNB) Model Trained Successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liR5JaSLdkK9",
        "outputId": "026c98cb-0497-478f-e650-8a1082a19b18"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Na誰ve Bayes (GaussianNB) Model Trained Successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Make Predictions\n",
        "y_pred = gnb_model.predict(X_test_scaled)\n",
        "y_pred_proba = gnb_model.predict_proba(X_test_scaled) # Probabilities for each class\n",
        "\n",
        "print(\"Predictions on Test Set (first 10):\")\n",
        "predictions_df = pd.DataFrame({\n",
        "    'Actual_Label': y_test.reset_index(drop=True),\n",
        "    'Actual_Species': [iris.target_names[label] for label in y_test.reset_index(drop=True)],\n",
        "    'Predicted_Label': y_pred,\n",
        "    'Predicted_Species': [iris.target_names[label] for label in y_pred]\n",
        "})\n",
        "print(predictions_df.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MR85eKLdkHs",
        "outputId": "85e6feb6-961b-4042-ced4-dfb4dd4df2eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions on Test Set (first 10):\n",
            "   Actual_Label Actual_Species  Predicted_Label Predicted_Species\n",
            "0             2      virginica                2         virginica\n",
            "1             1     versicolor                1        versicolor\n",
            "2             2      virginica                1        versicolor\n",
            "3             1     versicolor                1        versicolor\n",
            "4             2      virginica                2         virginica\n",
            "5             2      virginica                2         virginica\n",
            "6             1     versicolor                1        versicolor\n",
            "7             1     versicolor                1        versicolor\n",
            "8             0         setosa                0            setosa\n",
            "9             2      virginica                2         virginica\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Evaluate the Model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuwDws91d93s",
        "outputId": "957f0fe7-4763-440a-dec8-7727a6a7fbe5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.91\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        15\n",
            "  versicolor       0.82      0.93      0.88        15\n",
            "   virginica       0.92      0.80      0.86        15\n",
            "\n",
            "    accuracy                           0.91        45\n",
            "   macro avg       0.92      0.91      0.91        45\n",
            "weighted avg       0.92      0.91      0.91        45\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7NXYZTp4Wlg",
        "outputId": "3b99dc6a-7113-4bd5-a65b-c65bca2a74bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifying a New Sample with Na誰ve Bayes:\n",
            "\n",
            "New Sample to classify:\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0                5.1               3.5                1.4               0.2\n",
            "\n",
            "New Sample Scaled:\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0          -0.900681          1.019004          -1.340227         -1.315444\n",
            "\n",
            "The new sample is predicted to be: 'setosa'\n",
            "\n",
            "Probabilities for each species:\n",
            "  setosa: 1.0000\n",
            "  versicolor: 0.0000\n",
            "  virginica: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# 7. Classify a New Sample\n",
        "print(\"Classifying a New Sample with Na誰ve Bayes:\")\n",
        "\n",
        "# Example new sample: sepal length=5.1, sepal width=3.5, petal length=1.4, petal width=0.2 (Looks like a Setosa)\n",
        "new_sample = pd.DataFrame([[5.1, 3.5, 1.4, 0.2]], columns=iris.feature_names)\n",
        "print(f\"\\nNew Sample to classify:\\n{new_sample}\")\n",
        "\n",
        "# Crucially, scale the new sample using the *same* fitted scaler\n",
        "new_sample_scaled = scaler.transform(new_sample)\n",
        "new_sample_scaled_df = pd.DataFrame(new_sample_scaled, columns=iris.feature_names)\n",
        "\n",
        "print(f\"\\nNew Sample Scaled:\\n{new_sample_scaled_df}\")\n",
        "\n",
        "# Predict the class and probabilities for the new sample\n",
        "new_sample_prediction_label = gnb_model.predict(new_sample_scaled)[0]\n",
        "new_sample_prediction_species = iris.target_names[new_sample_prediction_label]\n",
        "new_sample_prediction_proba = gnb_model.predict_proba(new_sample_scaled)[0]\n",
        "\n",
        "print(f\"\\nThe new sample is predicted to be: '{new_sample_prediction_species}'\")\n",
        "print(\"\\nProbabilities for each species:\")\n",
        "for i, proba in enumerate(new_sample_prediction_proba):\n",
        "    print(f\"  {iris.target_names[i]}: {proba:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas scikit-learn nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbLwNYE5--Rb",
        "outputId": "2a32fca0-389a-40c1-a17b-98e2b01ac35f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# 1. Load Dataset\n",
        "df = pd.read_csv(\"SMSSpamCollection\", sep='\\t', names=[\"label\", \"message\"])\n",
        "\n",
        "# 2. Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove stopwords\n",
        "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "df['message'] = df['message'].apply(preprocess_text)\n",
        "\n",
        "# 3. Convert labels to binary (ham=0, spam=1)\n",
        "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# 4. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.3, random_state=42)\n",
        "\n",
        "# 5. Vectorization\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# 6. Train Na誰ve Bayes Classifier\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_vect, y_train)\n",
        "\n",
        "# 7. Predict and Evaluate\n",
        "y_pred = nb.predict(X_test_vect)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "H-I049X---Ns",
        "outputId": "f95d6338-5359-4211-cfdd-a11d48803966"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'SMSSpamCollection'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-300090221>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 1. Load Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SMSSpamCollection\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"message\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# 2. Preprocessing Function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SMSSpamCollection'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "# Download NLTK data if not already downloaded\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    WordNetLemmatizer().lemmatize('test')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "try:\n",
        "    nltk.data.find('corpora/omw-1.4')\n",
        "except LookupError:\n",
        "    nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "# --- 1. Load the Dataset ---\n",
        "# The dataset is typically a CSV file with two columns: 'v1' (label) and 'v2' (text)\n",
        "# Make sure to place 'SMSSpamCollection' in the same directory as your script,\n",
        "# or provide the full path to the file.\n",
        "try:\n",
        "    df = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['label', 'message'])\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(f\"Number of samples: {len(df)}\")\n",
        "    print(\"First 5 rows of the dataset:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nLabel distribution:\")\n",
        "    print(df['label'].value_counts())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'SMSSpamCollection' not found.\")\n",
        "    print(\"Please download the dataset from:\")\n",
        "    print(\"https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\")\n",
        "    print(\"And place it in the same directory as this script, or provide the full path.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Preprocessing the Text Data ---\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and preprocesses the text:\n",
        "    1. Removes non-alphabetic characters and multiple spaces.\n",
        "    2. Converts text to lowercase.\n",
        "    3. Tokenizes the text (splits into words).\n",
        "    4. Removes stop words.\n",
        "    5. Applies lemmatization (can also use stemming).\n",
        "    6. Joins the processed words back into a single string.\n",
        "    \"\"\"\n",
        "    # Remove non-alphabetic characters and extra spaces\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Replace multiple spaces with single space\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize\n",
        "    words = text.split()\n",
        "    # Remove stop words and apply lemmatization\n",
        "    processed_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    # Alternatively, use stemming:\n",
        "    # processed_words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(processed_words)\n",
        "\n",
        "# Apply preprocessing to the 'message' column\n",
        "print(\"\\nPreprocessing messages...\")\n",
        "df['processed_message'] = df['message'].apply(preprocess_text)\n",
        "print(\"Preprocessing complete. First 5 processed messages:\")\n",
        "print(df[['message', 'processed_message']].head())\n",
        "\n",
        "# --- 3. Feature Extraction (Vectorization) ---\n",
        "# Convert text messages into numerical feature vectors.\n",
        "# We'll use two common methods: CountVectorizer and TfidfVectorizer.\n",
        "\n",
        "# Option 1: CountVectorizer (Bag-of-Words)\n",
        "# This converts a collection of text documents to a matrix of token counts.\n",
        "print(\"\\nApplying CountVectorizer...\")\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_counts = count_vectorizer.fit_transform(df['processed_message'])\n",
        "print(f\"Shape of CountVectorizer features: {X_counts.shape}\")\n",
        "\n",
        "# Option 2: TfidfVectorizer (Term Frequency-Inverse Document Frequency)\n",
        "# This transforms text to feature vectors that reflect the importance of a word in a document\n",
        "# relative to the entire corpus.\n",
        "print(\"Applying TfidfVectorizer...\")\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['processed_message'])\n",
        "print(f\"Shape of TfidfVectorizer features: {X_tfidf.shape}\")\n",
        "\n",
        "# Choose one for the model. TF-IDF often performs better for text classification.\n",
        "X = X_tfidf\n",
        "# X = X_counts\n",
        "\n",
        "# Convert labels to numerical format (ham: 0, spam: 1)\n",
        "y = df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# --- 4. Split Data into Training and Testing Sets ---\n",
        "print(\"\\nSplitting data into training and testing sets (80/20 split)...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
        "\n",
        "# --- 5. Train the Na誰ve Bayes Classifier ---\n",
        "# We use Multinomial Na誰ve Bayes, which is well-suited for count-based features.\n",
        "print(\"\\nTraining Multinomial Na誰ve Bayes classifier...\")\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- 6. Make Predictions ---\n",
        "print(\"Making predictions on the test set...\")\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# --- 7. Evaluate the Model ---\n",
        "print(\"\\n--- Model Evaluation ---\")\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# --- 8. Test with Custom Messages ---\n",
        "print(\"\\n--- Testing with Custom Messages ---\")\n",
        "custom_messages = [\n",
        "    \"Congratulations! You've won a free iPhone. Click here to claim.\", # Spam\n",
        "    \"Hey, how are you doing today? Let's catch up soon.\",              # Ham\n",
        "    \"URGENT! Your account has been suspended. Verify your details now.\", # Spam\n",
        "    \"Hi, just confirming our meeting for tomorrow at 10 AM.\",          # Ham\n",
        "    \"Free entry to a contest! Text WIN to 12345.\",                     # Spam\n",
        "    \"Call me back please, it's urgent.\"                                # Ham (can be tricky)\n",
        "]\n",
        "\n",
        "# Preprocess and vectorize custom messages using the *trained* vectorizer\n",
        "# (Do not fit_transform again, only transform)\n",
        "processed_custom_messages = [preprocess_text(msg) for msg in custom_messages]\n",
        "X_custom = tfidf_vectorizer.transform(processed_custom_messages) # Use the same vectorizer as for training\n",
        "\n",
        "# Predict labels for custom messages\n",
        "predictions_custom = model.predict(X_custom)\n",
        "\n",
        "label_map = {0: 'ham', 1: 'spam'}\n",
        "for i, msg in enumerate(custom_messages):\n",
        "    predicted_label = label_map[predictions_custom[i]]\n",
        "    print(f\"Message: '{msg}'\\nPredicted: {predicted_label}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "x_Po5voN--JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hmkCeIg6--G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "920_2z3s--En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uFR_xA-g--Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "47I8krOI-9_9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}